{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Author: Ankit Kariryaa, University of Bremen\n",
    "\n",
    "   Modified by Jiawei Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-rc3\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import geopandas as gps\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "from rasterio import windows\n",
    "# import fiona                     # I/O vector data (shape, geojson, ...)\n",
    "# import geopandas as gps\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry import mapping, shape\n",
    "\n",
    "import numpy as np               # numerical array manipulation\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from itertools import product\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import sys\n",
    "from core.UNet import UNet\n",
    "from core.losses_FTL import focalTversky, accuracy, dice_coef, dice_loss, specificity, sensitivity, PA, IoU_Pos, IoU_Neg, mIoU, F1_Score\n",
    "from core.optimizers import adaDelta, adagrad, adam, nadam\n",
    "from core.frame_info import FrameInfo, image_normalize\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.split_frames import split_dataset\n",
    "from core.visualize import display_images\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "import matplotlib.patches as patches\n",
    "# from matplotlib.patches import Polygon\n",
    "\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto(\n",
    "    #device_count={\"CPU\": 64},\n",
    "    allow_soft_placement=True, \n",
    "    log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required configurations (including the input and output paths) are stored in a separate file (such as config/RasterAnalysis.py)\n",
    "# Please provide required info in the file before continuing with this notebook. \n",
    " \n",
    "from config import RasterAnalysis_withLocation\n",
    "# In case you are using a different folder name such as configLargeCluster, then you should import from the respective folder \n",
    "# Eg. from configLargeCluster import RasterAnalysis\n",
    "\n",
    "config = RasterAnalysis_withLocation.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model\n",
    "OPTIMIZER = adam\n",
    "LOSS = focalTversky\n",
    "# OPTIMIZER=tf.train.experimental.enable_mixed_precision_graph_rewrite(OPTIMIZER)\n",
    "import os\n",
    "model = load_model(config.trained_model_path, custom_objects={'focalTversky': LOSS, 'dice_coef': dice_coef, 'dice_loss':dice_loss, 'accuracy':accuracy , 'specificity': specificity, 'sensitivity':sensitivity, 'PA':PA, 'IoU_Pos':IoU_Pos, 'IoU_Neg':IoU_Neg, 'mIoU':mIoU, 'F1_Score':F1_Score}, compile=False)\n",
    "model.compile(optimizer=OPTIMIZER, loss=focalTversky, metrics=[dice_coef, dice_loss, accuracy, specificity, sensitivity, PA, IoU_Pos, IoU_Neg, mIoU, F1_Score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Location</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Drainage</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Start_date</th>\n",
       "      <th>End_date</th>\n",
       "      <th>CP1</th>\n",
       "      <th>CP2</th>\n",
       "      <th>CP3</th>\n",
       "      <th>CP4</th>\n",
       "      <th>CP5</th>\n",
       "      <th>CP6</th>\n",
       "      <th>CP7</th>\n",
       "      <th>CP8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DayaBay-Lingao</td>\n",
       "      <td>22.6076</td>\n",
       "      <td>114.5641</td>\n",
       "      <td>Bay</td>\n",
       "      <td>225</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>China</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>5802</td>\n",
       "      <td>2002-02-26</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2002-12-15</td>\n",
       "      <td>2010-07-15</td>\n",
       "      <td>2011-05-03</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yangjiang</td>\n",
       "      <td>21.7024</td>\n",
       "      <td>112.2713</td>\n",
       "      <td>Open</td>\n",
       "      <td>75</td>\n",
       "      <td>Deep</td>\n",
       "      <td>China</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>6000</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changjiang</td>\n",
       "      <td>19.4737</td>\n",
       "      <td>108.8834</td>\n",
       "      <td>Open</td>\n",
       "      <td>175</td>\n",
       "      <td>Deep</td>\n",
       "      <td>China</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>1202</td>\n",
       "      <td>2015-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-06-20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fuqing</td>\n",
       "      <td>25.4152</td>\n",
       "      <td>119.4365</td>\n",
       "      <td>Bay</td>\n",
       "      <td>125</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>China</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>5000</td>\n",
       "      <td>2014-08-20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-08-06</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ningde</td>\n",
       "      <td>27.0524</td>\n",
       "      <td>120.2868</td>\n",
       "      <td>Bay</td>\n",
       "      <td>275</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>China</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>4072</td>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>2015-03-21</td>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Point_Beach</td>\n",
       "      <td>44.2822</td>\n",
       "      <td>-87.5327</td>\n",
       "      <td>Lake</td>\n",
       "      <td>175</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>1182</td>\n",
       "      <td>1970-11-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1972-08-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Robert_E_Ginna</td>\n",
       "      <td>43.2805</td>\n",
       "      <td>-77.3082</td>\n",
       "      <td>Lake</td>\n",
       "      <td>125</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>560</td>\n",
       "      <td>1969-12-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Zion</td>\n",
       "      <td>42.4450</td>\n",
       "      <td>-87.7982</td>\n",
       "      <td>Lake</td>\n",
       "      <td>125</td>\n",
       "      <td>Deep</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>2080</td>\n",
       "      <td>1973-06-28</td>\n",
       "      <td>1998-02-13</td>\n",
       "      <td>1973-12-26</td>\n",
       "      <td>1998-02-13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Bruce_1</td>\n",
       "      <td>44.3448</td>\n",
       "      <td>-81.5784</td>\n",
       "      <td>Lake</td>\n",
       "      <td>275</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>6358</td>\n",
       "      <td>1977-01-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1976-09-04</td>\n",
       "      <td>1977-12-12</td>\n",
       "      <td>1978-12-21</td>\n",
       "      <td>1984-12-02</td>\n",
       "      <td>1984-06-26</td>\n",
       "      <td>1986-02-22</td>\n",
       "      <td>1987-03-09</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Bruce_2</td>\n",
       "      <td>44.3158</td>\n",
       "      <td>-81.6116</td>\n",
       "      <td>Lake</td>\n",
       "      <td>125</td>\n",
       "      <td>Shallow</td>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>6358</td>\n",
       "      <td>1977-01-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1976-09-04</td>\n",
       "      <td>1977-12-12</td>\n",
       "      <td>1978-12-21</td>\n",
       "      <td>1984-12-02</td>\n",
       "      <td>1984-06-26</td>\n",
       "      <td>1986-02-22</td>\n",
       "      <td>1987-03-09</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name      Lat       Lon Location  Radius Drainage  \\\n",
       "0   DayaBay-Lingao  22.6076  114.5641      Bay     225  Shallow   \n",
       "1        Yangjiang  21.7024  112.2713     Open      75     Deep   \n",
       "2       Changjiang  19.4737  108.8834     Open     175     Deep   \n",
       "3           Fuqing  25.4152  119.4365      Bay     125  Shallow   \n",
       "4           Ningde  27.0524  120.2868      Bay     275  Shallow   \n",
       "..             ...      ...       ...      ...     ...      ...   \n",
       "69     Point_Beach  44.2822  -87.5327     Lake     175  Shallow   \n",
       "70  Robert_E_Ginna  43.2805  -77.3082     Lake     125  Shallow   \n",
       "71            Zion  42.4450  -87.7982     Lake     125     Deep   \n",
       "72         Bruce_1  44.3448  -81.5784     Lake     275  Shallow   \n",
       "73         Bruce_2  44.3158  -81.6116     Lake     125  Shallow   \n",
       "\n",
       "          Country         Region  Capacity Start_date   End_date        CP1  \\\n",
       "0           China      East Asia      5802 2002-02-26        NaT 2002-12-15   \n",
       "1           China      East Asia      6000 2013-12-31        NaT 2015-03-10   \n",
       "2           China      East Asia      1202 2015-11-07        NaT 2016-06-20   \n",
       "3           China      East Asia      5000 2014-08-20        NaT 2015-08-06   \n",
       "4           China      East Asia      4072 2012-12-28        NaT 2014-01-04   \n",
       "..            ...            ...       ...        ...        ...        ...   \n",
       "69  United States  North America      1182 1970-11-06        NaT 1972-08-02   \n",
       "70  United States  North America       560 1969-12-02        NaT        NaT   \n",
       "71  United States  North America      2080 1973-06-28 1998-02-13 1973-12-26   \n",
       "72         Canada  North America      6358 1977-01-14        NaT 1976-09-04   \n",
       "73         Canada  North America      6358 1977-01-14        NaT 1976-09-04   \n",
       "\n",
       "          CP2        CP3        CP4        CP5        CP6        CP7 CP8  \n",
       "0  2010-07-15 2011-05-03        NaT        NaT        NaT        NaT NaT  \n",
       "1  2015-10-18 2017-01-08 2018-05-23 2019-06-29        NaT        NaT NaT  \n",
       "2         NaT        NaT        NaT        NaT        NaT        NaT NaT  \n",
       "3  2016-09-07 2017-07-29 2020-11-27        NaT        NaT        NaT NaT  \n",
       "4  2015-03-21 2016-03-29        NaT        NaT        NaT        NaT NaT  \n",
       "..        ...        ...        ...        ...        ...        ...  ..  \n",
       "69        NaT        NaT        NaT        NaT        NaT        NaT NaT  \n",
       "70        NaT        NaT        NaT        NaT        NaT        NaT NaT  \n",
       "71 1998-02-13        NaT        NaT        NaT        NaT        NaT NaT  \n",
       "72 1977-12-12 1978-12-21 1984-12-02 1984-06-26 1986-02-22 1987-03-09 NaT  \n",
       "73 1977-12-12 1978-12-21 1984-12-02 1984-06-26 1986-02-22 1987-03-09 NaT  \n",
       "\n",
       "[74 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "location_df = pd.read_excel(r'I:\\results\\SST\\landsat\\location.xlsx')\n",
    "location_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to add results of a patch to the total results of a larger area. The operator could be min (useful if there are too many false positives), max (useful for tackle false negatives)\n",
    "def addTOResult(res, prediction, row, col, he, wi, operator = 'MAX'):\n",
    "    currValue = res[row:row+he, col:col+wi]\n",
    "    newPredictions = prediction[:he, :wi]\n",
    "# IMPORTANT: MIN can't be used as long as the mask is initialed with 0!!!!! If you want to use MIN initial the mask with -1 and handle the case of default value(-1) separately.\n",
    "    if operator == 'MIN': # Takes the min of current prediction and new prediction for each pixel\n",
    "        currValue [currValue == -1] = 1 #Replace -1 with 1 in case of MIN\n",
    "        resultant = np.minimum(currValue, newPredictions) \n",
    "    elif operator == 'MAX':\n",
    "        resultant = np.maximum(currValue, newPredictions)\n",
    "    elif operator == 'REPLACE':\n",
    "        resultant = newPredictions    \n",
    "# Alternative approach; Lets assume that quality of prediction is better in the centre of the image than on the edges\n",
    "# We use numbers from 1-5 to denote the quality, where 5 is the best and 1 is the worst.In that case, the best result would be to take into quality of prediction based upon position in account\n",
    "# So for merge with stride of 0.5, for eg. [12345432100000] AND [00000123454321], should be [1234543454321] instead of [1234543214321] that you will currently get. \n",
    "# However, in case the values are strecthed before hand this problem will be minimized\n",
    "    res[row:row+he, col:col+wi] =  resultant\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods that actually makes the predictions\n",
    "def predict_using_model(model, batch, batch_pos, mask, operator):\n",
    "    tm = np.stack(batch, axis = 0)\n",
    "#     print('tm', tm.shape)\n",
    "    prediction = model.predict(tm)\n",
    "#     print('prediction', prediction.shape)\n",
    "    for i in range(len(batch_pos)):\n",
    "        (col, row, wi, he) = batch_pos[i]\n",
    "        p = np.squeeze(prediction[i], axis = -1)\n",
    "#         print('p', p.shape)\n",
    "        # Instead of replacing the current values with new values, use the user specified operator (MIN,MAX,REPLACE)\n",
    "        mask = addTOResult(mask, p, row, col, he, wi, operator)\n",
    "    return mask\n",
    "    \n",
    "\n",
    "def detect_plume(WST_img, width=256, height=256, stride = 128, normalize=False): \n",
    "    nols, nrows = WST_img.meta['width'], WST_img.meta['height']\n",
    "    meta = WST_img.meta.copy()\n",
    "    if 'float' not in meta['dtype']: #The prediction is a float so we keep it as float to be consistent with the prediction. \n",
    "        meta['dtype'] = np.float32\n",
    "    offsets = product(range(0, nols, stride), range(0, nrows, stride))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "#     print(nrows, nols)\n",
    "    print('the size of current cWST_img',nrows, nols) #（40000，40000）\n",
    "    \n",
    "    mask = np.zeros((nrows, nols), dtype=meta['dtype'])\n",
    "\n",
    "#     mask = mask -1 # Note: The initial mask is initialized with -1 instead of zero to handle the MIN case (see addToResult)\n",
    "    batch = []\n",
    "    batch_pos = [ ]\n",
    "    for col_off, row_off in  tqdm(offsets):\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, WST_img.transform)\n",
    "        patch = np.zeros((height, width, 2)) #Add zero padding in case of corner images00\n",
    "        WST_sm = WST_img.read(window=window)\n",
    "        WST_sm = np.nan_to_num(WST_sm, nan=-255)\n",
    "#         print('WST_sm', WST_sm.shape)\n",
    "        temp_im = np.stack(WST_sm, axis = -1) \n",
    "#         print('temp_im', temp_im.shape)\n",
    "#         temp_im = np.squeeze(temp_im)\n",
    "        \n",
    "        if normalize:\n",
    "            temp_im = image_normalize(temp_im, axis=(0,1)) # Normalize the image along the width and height i.e. independently per channel\n",
    "            \n",
    "        patch[:window.height, :window.width] = temp_im\n",
    "#         print('patch', patch.shape)\n",
    "        batch.append(patch)\n",
    "        batch_pos.append((window.col_off, window.row_off, window.width, window.height))\n",
    "        if (len(batch) == config.BATCH_SIZE):\n",
    "            mask = predict_using_model(model, batch, batch_pos, mask, 'MAX')\n",
    "            batch = []\n",
    "            batch_pos = []\n",
    "            \n",
    "    # To handle the edge of images as the image size may not be divisible by n complete batches and few frames on the edge may be left.\n",
    "    if batch:\n",
    "        mask = predict_using_model(model, batch, batch_pos, mask, 'MAX')\n",
    "        batch = []\n",
    "        batch_pos = []\n",
    "\n",
    "    return(mask, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import defaultdict\n",
    "import rasterio.features\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'id': 'str', 'plume': 'float:15.2',},\n",
    "    }\n",
    "\n",
    "# Generate a mask with polygons\n",
    "def transformContoursToXY(contours, transform): # = None\n",
    "    tp = []\n",
    "    for cnt in contours:\n",
    "        pl = cnt[:, 0, :]\n",
    "        cols, rows = zip(*pl)\n",
    "        x,y = rasterio.transform.xy(transform, rows, cols)\n",
    "        if type(x)==np.float64:\n",
    "            print(cols, rows, x, y, pl)\n",
    "            tl = [(x, y)]\n",
    "        else:\n",
    "            tl = [list(i) for i in zip(x, y)]\n",
    "        tp.append(tl)\n",
    "    return (tp)\n",
    "\n",
    "def mask_to_polygons(j, lon, lat, maskF, transform):\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    th = 0.5\n",
    "    mask = maskF.copy()\n",
    "    mask[mask < th] = 0\n",
    "    mask[mask >= th] = 1\n",
    "    mask = ((mask) * 255).astype(np.uint8)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #Convert contours from image coordinate to xy coordinate\n",
    "    contours = transformContoursToXY(contours, transform)\n",
    "    if not contours: #TODO: Raise an error maybe\n",
    "        print('Warning: No contours/polygons detected!!')\n",
    "        print(maskF.max())\n",
    "        return [Polygon()]\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(contours[idx])\n",
    "\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    all_polygons_area = []\n",
    "    for idx, cnt in enumerate(contours):\n",
    "        if idx not in child_contours and len(cnt)>=3: #and cv2.contourArea(cnt) >= min_area: #Do we need to check for min_area?? and cv2.pointPolygonTest(np.float32(cnt), (lon,lat), False) != -1\n",
    "            try:\n",
    "\n",
    "                poly = Polygon(\n",
    "                    shell=cnt,\n",
    "                    holes=[c for c in cnt_children.get(idx, [])]) #\n",
    "                           #if cv2.contourArea(c) >= min_area]) #Do we need to check for min_area??\n",
    "                \n",
    "                point = Point(lon,lat)\n",
    "#                 print(point)\n",
    "#                 print(poly)\n",
    "#                 if poly.intersects(point.buffer(0.01)):\n",
    "                all_polygons.append(poly)\n",
    "                all_polygons_area.append(poly.area)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "#                 print(\"An exception occurred in createShapefileObject; Polygon must have more than 2 points\")\n",
    "#     if j == 7:\n",
    "#          print(all_polygons[0])\n",
    "#     print(len(all_polygons))\n",
    "#     print(*all_polygons, sep='\\n')\n",
    "    if len(all_polygons)>1:\n",
    "        max_poly = max(all_polygons_area)\n",
    "        idx_max = all_polygons_area.index(max_poly)\n",
    "        all_polygons = [all_polygons[idx_max]]\n",
    "#         dist_list = []\n",
    "#         for idx, poly in enumerate(all_polygons):\n",
    "#             dist = point.distance(poly)\n",
    "#             dist_list.append(dist)\n",
    "#         min_value = min(dist_list)\n",
    "#         print(min_value)\n",
    "#         min_idx = dist_list.index(min_value)\n",
    "#         all_polygons = [all_polygons[min_idx]]\n",
    "#         if min_value > 0.01:\n",
    "#             all_polygons = [Polygon()]\n",
    "        \n",
    "    if len(all_polygons)==0:\n",
    "        all_polygons = [Polygon()]\n",
    "#     print(len(all_polygons))\n",
    "#     print(*all_polygons, sep='\\n')\n",
    "    return(all_polygons)\n",
    "\n",
    "\n",
    "def create_contours_shapefile(j, lon, lat, mask, meta, out_fn):\n",
    "    res = mask_to_polygons(j, lon, lat, mask, meta['transform'])\n",
    "#     res = transformToXY(contours, meta['transform'])\n",
    "#     createShapefileObject(res, meta, out_fn)\n",
    "    if not res[0].is_empty:\n",
    "        location_mask = rasterio.features.rasterize(res, fill=0, out_shape=mask.shape, transform=meta['transform'], all_touched=True, default_value=1, dtype='uint8')\n",
    "    else:\n",
    "        location_mask = np.zeros(mask.shape)\n",
    "    \n",
    "    return(location_mask)\n",
    "\n",
    "\n",
    "def writeMaskToDisk(j, lon, lat, WST, label, station_name, time_str, detected_mask, detected_meta, wp, write_as_type = 'uint8', th = 0.5, create_countors = True): #uint8\n",
    "    # Convert to correct required before writing\n",
    "    if 'float' in str(detected_meta['dtype']) and 'int' in write_as_type:\n",
    "        print(f'Converting prediction from {detected_meta[\"dtype\"]} to {write_as_type}, using threshold of {th}')\n",
    "        detected_mask[detected_mask<th]=0\n",
    "        detected_mask[detected_mask>=th]=1\n",
    "\n",
    "        label_arr=label.read()\n",
    "        label_arr=np.squeeze(label_arr, axis=0)\n",
    "        \n",
    "#         print(label_arr.shape)\n",
    "\n",
    "        \n",
    "        detected_mask[label_arr==0]=0  #mask pred results outside of the label\n",
    "\n",
    "        \n",
    "        detected_mask = detected_mask.astype(write_as_type)\n",
    "        WST_arr=WST.read()[0]\n",
    "#         print(WST_arr.shape)\n",
    "        \n",
    "#         WST_arr=np.squeeze(WST_arr, axis=0)\n",
    "        WST_arr[detected_mask==0]=np.nan\n",
    "        WST_arr[WST_arr==0]=np.nan\n",
    "\n",
    "        if station_name == 'Ohi':\n",
    "            Ohi_mask = np.zeros(WST_arr.shape, dtype=write_as_type)\n",
    "            Ohi_mask[WST_arr>10] = 1\n",
    "            detected_mask[Ohi_mask==1] = 0\n",
    "        elif station_name == 'Dungeness' and time_str == '19890630':\n",
    "            detected_mask[:, :]= 0\n",
    "        elif station_name == 'Vandellos' and time_str == '20100314':\n",
    "            detected_mask[:, :]= 0\n",
    "            \n",
    "        print(detected_mask.dtype)\n",
    "        detected_meta['dtype']=write_as_type\n",
    "        \n",
    "#         #transfer crs\n",
    "#         src_crs=WST.crs\n",
    "#         dst_crs={'init': 'EPSG:8857'} #equal earth projection\n",
    "#         src_transform=WST.transform\n",
    "#         dst_transform, width, height=cdt(src_crs, dst_crs, WST.width, WST.height, *WST.bounds)\n",
    "#         kwargs=WST.meta.copy()\n",
    "#         kwargs.update({\n",
    "#             'crs': dst_crs,\n",
    "#             'transform': dst_transform,\n",
    "#             'width': width,\n",
    "#             'height': height})\n",
    "        \n",
    "    \n",
    "#     if create_countors:\n",
    "#     wp = wp.replace(config.output_image_type, config.output_shapefile_type)\n",
    "    location_mask = create_contours_shapefile(j, lon, lat, detected_mask, detected_meta, wp)\n",
    "    location_mask = location_mask.astype(np.float32)\n",
    "    if np.all((location_mask == 0)):\n",
    "        WST_arr[:,:] = np.nan\n",
    "    else:\n",
    "        WST_arr[location_mask == 0] = np.nan\n",
    "    \n",
    "    \n",
    "    if station_name == 'Ohi':\n",
    "        WST_arr[WST_arr > 10] = np.nan \n",
    "    \n",
    "    detected_meta['dtype'] = np.float32\n",
    "    detected_meta['count'] = 1\n",
    "    with rasterio.open(wp, 'w', **detected_meta) as outds:\n",
    "        outds.write(WST_arr, 1)\n",
    "#         reproject(\n",
    "#             source=WST_arr,\n",
    "#             destination=destination,\n",
    "#             src_transform=src_transform,\n",
    "#             src_crs=src_crs,\n",
    "#             dst_transform=dst_transform,\n",
    "#             dst_crs=dst_crs,\n",
    "#             resampling=Resampling.nearest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict plumes in the all the files in the input image dir\n",
    "# Depending upon the available RAM, images may not to be split before running this cell.\n",
    "# Use the Auxiliary-2-SplitRasterToAnalyse if the images are too big to be analysed in memory.\n",
    "all_files = []\n",
    "all_labels = []\n",
    "for root, dirs, files in os.walk(config.input_image_dir):\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if file.endswith(config.input_image_type) and file.startswith(config.WST_fn_st):\n",
    "             all_files.append((os.path.join(root, file), file))\n",
    "        if file.endswith(config.input_image_type) and file.startswith(config.label_fn_st):\n",
    "             all_labels.append((os.path.join(root, file), file))\n",
    "pd.Series(all_files, all_labels)\n",
    "for idx, (fullPath, filename) in enumerate(all_files):\n",
    "    lb_path, lb_fn = all_labels[idx]\n",
    "    outputFile = os.path.join(config.output_dir, filename.replace(config.WST_fn_st, config.output_prefix) )\n",
    "    if not os.path.isfile(outputFile) or config.overwrite_analysed_files: #isfile function check whether the path is existing or not\n",
    "        with rasterio.open(fullPath) as WST: \n",
    "            with rasterio.open(lb_path) as label:\n",
    "                label.meta['dtype']=np.uint8 \n",
    "                idImg_sub1 = filename.find('_', 6)\n",
    "                idImg_sub2 = filename.rfind('L')\n",
    "                station_name = filename[idImg_sub1+1:idImg_sub2-1]\n",
    "                time_str = filename[-12:-4]\n",
    "                print(filename)\n",
    "                lon = location_df.loc[location_df['Name']==station_name,['Lon']].values[0][0]\n",
    "                lat = location_df.loc[location_df['Name']==station_name,['Lat']].values[0][0]\n",
    "                detectedMask, detectedMeta = detect_plume(WST, width = config.WIDTH, height = config.HEIGHT, stride = config.STRIDE, normalize = False) # WIDTH and HEIGHT should be the same and in this case Stride is 50 % width\n",
    "\n",
    "        #Write the mask to file \n",
    "                writeMaskToDisk(idx, lon, lat, WST, label, station_name, time_str, detectedMask, detectedMeta, outputFile, write_as_type = config.output_dtype, th = 0.5, create_countors = False)\n",
    "                \n",
    "    else:\n",
    "        print('File already analysed!', fullPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Display extracted image\n",
    "sampleImage = ''\n",
    "fn = os.path.join(config.output_dir, config.output_prefix + sampleImage )\n",
    "predicted_img = rasterio.open(fn)\n",
    "p = predicted_img.read()\n",
    "np.unique(p, return_counts=True)\n",
    "plt.imshow(p[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
